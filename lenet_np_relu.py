# -*- coding: utf-8 -*-
"""lenet_np.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bwCEpLqUyaCTrWASWO55dE6MnGITNJAs
"""

# import torch
import torchvision
import numpy as np
import os

# --- 1. 下载和加载数据 ---

# 定义数据下载和存储的路径
data_dir = './mnist_data'

# 定义批次大小
BATCH_SIZE = 128
epochs = 100

# 加载训练集
train_dataset = torchvision.datasets.MNIST(
    root=data_dir,
    train=True,
    download=True,
    transform=None  # 不使用 ToTensor()
)

X_train_np = train_dataset.data.float().numpy() / 255.0

# 标签数据 (Y)
Y_train_np = train_dataset.targets.numpy()

# 使用 np.expand_dims 在索引 1 (Channel 维度) 处增加一个维度
# 形状: (60000, 28, 28) -> (60000, 1, 28, 28)
X_train_final = np.expand_dims(X_train_np, axis=1)


def print_grad_stats(name, arr):
    arr = np.array(arr)  # 防止 view 问题
    print(f"{name:20s} shape={arr.shape} mean={arr.mean():.3e} std={arr.std():.3e} min={arr.min():.3e} max={arr.max():.3e} any_nan={np.isnan(arr).any()} any_inf={np.isinf(arr).any()}")

def create_numpy_batches(X_data, Y_data, batch_size, shuffle=True):
    data_size = len(X_data)

    # 1. (可选) 打乱索引
    if shuffle:
        indices = np.arange(data_size)
        np.random.shuffle(indices)
        X_data = X_data[indices]
        Y_data = Y_data[indices]

    # 2. 迭代生成批次并计数
    batch_idx = 0
    for i in range(0, data_size, batch_size):
        end_index = min(i + batch_size, data_size)

        X_batch = X_data[i:end_index]
        Y_batch = Y_data[i:end_index]

        # 返回批次索引、数据和标签
        yield (batch_idx, X_batch, Y_batch)

        # 索引递增
        batch_idx += 1


print(f"原始 NumPy 形状: {train_dataset.data.float().numpy().shape}")
print(f"最终 NumPy 形状 (Batch, 1, 28, 28): {X_train_final.shape}")
print(f"标签形状: {Y_train_np.shape}")
print(f"数据类型 (图像): {X_train_final.dtype}")
print("-" * 30)

batch_generator = create_numpy_batches(X_train_final, Y_train_np, BATCH_SIZE)
_, first_X_batch, first_Y_batch = next(batch_generator)

print(f"成功获取第一个 NumPy Batch (BATCH_SIZE={BATCH_SIZE}):")
print(f"Batch 图像数据 (X_batch) 形状: {first_X_batch.shape}")
print(f"Batch 标签数据 (Y_batch) 形状: {first_Y_batch.shape}")
print(f"Batch 图像数据类型: {first_X_batch.dtype}")
print("-" * 30)


def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# 更稳定的 loss（直接用 logits z3，避免先计算 y_hat）
def cross_entropy_from_logits(z, y_onehot):
    # z: (batch, C), y_onehot: (batch, C)
    z_max = np.max(z, axis=1, keepdims=True)
    log_sum_exp = np.log(np.sum(np.exp(z - z_max), axis=1, keepdims=True)) + z_max
    # note: -sum(y * z) + sum(log_sum_exp)  -> average later
    loss = (-np.sum(y_onehot * z, axis=1, keepdims=True) + log_sum_exp).mean()
    return loss

def softmax(z):
    z_shift = z - np.max(z, axis=1, keepdims=True)
    ex = np.exp(z_shift)
    return ex / np.sum(ex, axis=1, keepdims=True)

def relu(x):
    return x * (x > 0).astype(x.dtype)

def softmax_jacobian(x):
    if len(x.shape) == 1:
        return np.diag(x) - np.outer(x, x)
    else:
        diag = np.zeros((x.shape[0], x.shape[1], x.shape[1]))
        diag[:, np.arange(x.shape[1]), np.arange(x.shape[1])] = x
        outer = x[..., None, :] * x[...,:, None]
        return diag - outer

lr = 0.001
output_size = 10
S4_c = 16
S2_c = 6

c1_stride = 1
c1_padding = 2

S2_size = 28
S2_stride = 2
S2_p_size = 2

c3_stride = 1
c3_padding = 0

S4_size = 10
S4_stride = 2
S4_p_size = 2

c1_b = np.zeros(S2_c, dtype=np.float32)
c3_b = np.zeros(S4_c, dtype=np.float32)

# float32 Xavier/Glorot 初始化
def xavier_normal_scale(fan_in, fan_out):
    return np.sqrt(2 / (fan_in + fan_out))

fan_in_c1 = 1 * 5 * 5
fan_out_c1 = S2_c * 5 * 5
std_c1 = xavier_normal_scale(fan_in_c1, fan_out_c1)
c1_conv = np.random.randn(S2_c, 1, 5, 5).astype(np.float32) * std_c1 # 使用 astype 或 dtype=np.float32

fan_in_c3 = S2_c * 5 * 5
fan_out_c3 = S4_c * 5 * 5
std_c3 = xavier_normal_scale(fan_in_c3, fan_out_c3)
c3_conv = np.random.randn(S4_c, S2_c, 5, 5).astype(np.float32) * std_c3

std_w1 = xavier_normal_scale(400, 120)
W1 = np.random.randn(400, 120).astype(np.float32) * std_w1
b1 = np.zeros(120,).astype(np.float32)

std_w2 = xavier_normal_scale(120, 84)
W2 = np.random.randn(120, 84).astype(np.float32) * std_w2
b2 = np.zeros(84,).astype(np.float32)

std_w3 = xavier_normal_scale(84, 10)
W3 = np.random.randn(84, 10).astype(np.float32) * std_w3
b3 = np.zeros(10,).astype(np.float32)
for epoch in range(1, epochs + 1):
    for batch_index, X_batch, Y_batch in create_numpy_batches(X_train_final, Y_train_np, BATCH_SIZE):
        batch_size = X_batch.shape[0]
        # 将 NumPy 数组转换为 PyTorch Tensor (如果需要输入模型)
        c1_x = X_batch
        c1_x_b, c1_x_c, c1_x_h, c1_x_w = c1_x.shape
        c1_c_b, c1_c_c, c1_c_h, c1_c_w = c1_conv.shape
        c1_x_h += 2 * c1_padding
        c1_x_w += 2 * c1_padding
        c1_X = np.zeros((c1_x_b, c1_x_c, c1_x_h, c1_x_w))
        c1_X[:,:, c1_padding:-c1_padding, c1_padding:-c1_padding] = c1_x

        c1_h_pos = np.arange(0, c1_x_h - c1_c_h + 1, c1_stride)[:,None,None,None]
        c1_w_pos = np.arange(0, c1_x_w - c1_c_w + 1, c1_stride)[:,None,None]
        c1_I, c1_J = np.meshgrid(np.arange(c1_c_h), np.arange(c1_c_w), indexing='ij')

        # Implemented as an explicit loop over the input/filter elements (unrolled using broadcasting/indexing)
        # c1_z shape: (batch_size, num_filters, input_channels, output_h, output_w)
        c1_z = (c1_X[:, np.newaxis, :, c1_h_pos+c1_I, c1_w_pos+c1_J]
                * c1_conv[:, :,np.newaxis,np.newaxis,:,:]).sum(axis=(-2, -1))
        c1_z = np.sum(c1_z, axis=2)
        c1_z += c1_b[:,None,None]
        # print(c1_z.shape) (32, 6, 28, 28) -> batch, channel, H, W

        S2_in = relu(c1_z)
        S2_I, S2_J = np.meshgrid(np.arange(S2_p_size), np.arange(S2_p_size), indexing='ij')
        S2_I, S2_J = np.arange(0,S2_size,S2_stride)[:,None,None,None] + S2_I, np.arange(0,S2_size,S2_stride)[:,None,None] + S2_J
        S2_devided = S2_in[:,:,S2_I,S2_J].copy()
        S2_out = S2_in[:,:,S2_I,S2_J].max(axis=(-2,-1))

        c3_x = S2_out
        c3_x_b, c3_x_c, c3_x_h, c3_x_w = c3_x.shape
        c3_c_b, c3_c_c, c3_c_h, c3_c_w = c3_conv.shape

        c3_padded_h = c3_x_h + 2 * c3_padding
        c3_padded_w = c3_x_w + 2 * c3_padding
        c3_X_padded = np.zeros((c3_x_b, c3_x_c, c3_padded_h, c3_padded_w))
        c3_X_padded[:, :, c3_padding : c3_padded_h - c3_padding, c3_padding : c3_padded_w - c3_padding] = c3_x
        c3_H_out = (c3_padded_h - c3_c_h) // c3_stride + 1
        c3_W_out = (c3_padded_w - c3_c_w) // c3_stride + 1

        # 2.1 索引创建
        c3_h_pos = np.arange(0, c3_padded_h - c3_c_h + 1, c3_stride)[:, None, None, None]
        c3_w_pos = np.arange(0, c3_padded_w - c3_c_w + 1, c3_stride)[:, None, None]
        c3_I, c3_J = np.meshgrid(np.arange(c3_c_h), np.arange(c3_c_w), indexing='ij')
        c3_H_index = c3_h_pos + c3_I
        c3_W_index = c3_w_pos + c3_J

        # 2.2 卷积计算
        c3_X_blocks = c3_X_padded[:, np.newaxis, :, c3_H_index, c3_W_index]
        c3_conv_reshaped = c3_conv[:, :, np.newaxis, np.newaxis, :, :]
        c3_Z_mult = c3_X_blocks * c3_conv_reshaped
        c3_z = c3_Z_mult.sum(axis=(-2, -1)) # 沿着 F_h, F_w 求和 (输入通道 C_in 仍保留)
        c3_Z_out = np.sum(c3_z, axis=2)
        c3_Z_out += c3_b[:,None,None]

        S4_in = relu(c3_Z_out)
        S4_I, S4_J = np.meshgrid(np.arange(S4_p_size), np.arange(S4_p_size), indexing='ij')
        S4_I, S4_J = np.arange(0,S4_size,S4_stride)[:,None,None,None] + S4_I, np.arange(0,S4_size,S4_stride)[:,None,None] + S4_J
        S4_out = S4_in[:,:,S4_I,S4_J].mean(axis=(-2,-1))

        X1 = S4_out.reshape(batch_size,-1)
        z1 = X1 @ W1 + b1
        X2 = relu(z1)
        z2 = X2 @ W2 + b2
        X3 = relu(z2)
        z3 = X3 @ W3 + b3
        y_hat = softmax(z3)
        y = np.zeros((batch_size,output_size))
        y[np.arange(batch_size),Y_batch] = 1

        loss = np.sum(y * (-np.log(y_hat + 1e-8)),axis=-1).mean()
        if batch_index % 20 == 0:
            print(f'epoch: {epoch}, batch_index: {batch_index}, loss: {loss}')
        z3_loss = y_hat - y
        b3_loss = z3_loss.sum(axis=0)
        W3_loss = np.matmul(X3.T,z3_loss)
        X3_loss = np.matmul(z3_loss,W3.T)
        z2_loss = X3_loss * (z2 > 0).astype(np.float32())
        b2_loss = z2_loss.sum(axis=0)
        W2_loss = np.matmul(X2.T,z2_loss)
        X2_loss = np.matmul(z2_loss,W2.T)
        z1_loss = X2_loss * (z1 > 0).astype(np.float32())
        b1_loss = z1_loss.sum(axis=0)
        W1_loss = np.matmul(X1.T,z1_loss)
        X1_loss = np.matmul(z1_loss,W1.T)
        temp_loss = X1_loss.reshape(batch_size, S4_c, S4_size // S4_stride, S4_size // S4_stride)
        S4_loss = (temp_loss / (S4_p_size * S4_p_size)).repeat(2, -1).repeat(2, -2)
        # print(S4_loss[0,0,0,:5])
        # print(W1_loss[0][:5])
        # print(b1_loss[:5])

        c3_z_b, c3_z_c, c3_z_h, c3_z_w = c3_Z_out.shape
        c3_l = (c3_Z_out > 0).astype(np.float32()) * S4_loss

        # 重新构造索引，用于提取输入数据块 X 和 L
        c3_idx_x_h = np.arange(0, c3_padded_h - c3_c_h + 1, c3_stride)
        c3_idx_x_w = np.arange(0, c3_padded_w - c3_c_w + 1, c3_stride)
        c3_idx_x_I, c3_idx_x_J = np.meshgrid(c3_idx_x_h, c3_idx_x_w, indexing='ij')

        c3_rel_I, c3_rel_J = np.meshgrid(np.arange(c3_c_h), np.arange(c3_c_w), indexing='ij')

        c3_final_I = c3_rel_I[..., None, None] + c3_idx_x_I
        c3_final_J = c3_rel_J[..., None, None] + c3_idx_x_J

        c3_L_reshaped = c3_l[:, :, np.newaxis, np.newaxis, np.newaxis, :, :]

        c3_X_blocks_for_grad = c3_X_padded[:, np.newaxis, :, c3_final_I, c3_final_J]
        c3_grad_mult = c3_X_blocks_for_grad * c3_L_reshaped
        c3_conv_grad = c3_grad_mult.sum(axis=(-2,-1)).sum(axis=0)

        c3_b_grad = c3_l.sum(axis=(-2,-1)).sum(axis=0)

        # 计算需要的填充量
        c3_pad_up = c3_c_h - 1 - c3_padding
        c3_pad_left = c3_c_w - 1 - c3_padding
        c3_l_pad = np.zeros((c3_z_b, c3_z_c,
                             c3_x_h + c3_c_h - 2 * c3_padding - 1,
                             c3_x_w + c3_c_w - 2 * c3_padding - 1))

        # 创建 L 放置到 L_pad 的索引
        c3_l_pad_I, c3_l_pad_J = np.meshgrid(
            np.arange(c3_pad_up, c3_pad_up + c3_stride * c3_z_h, c3_stride),
            np.arange(c3_pad_left, c3_pad_left + c3_stride * c3_z_w, c3_stride),
            indexing='ij'
        )

        c3_l_pad[:, :, c3_l_pad_I, c3_l_pad_J] = c3_l

        c3_l_pad_conv_I, c3_l_pad_conv_J = np.meshgrid(np.arange(c3_c_h), np.arange(c3_c_w), indexing='ij')
        c3_l_pad_H_index = np.arange(c3_x_h - 2 * c3_padding)[:, None, None, None] + c3_l_pad_conv_I
        c3_l_pad_W_index = np.arange(c3_x_w - 2 * c3_padding)[:, None, None] + c3_l_pad_conv_J
        c3_L_pad_blocks = c3_l_pad[:, :, np.newaxis, c3_l_pad_H_index, c3_l_pad_W_index]

        # 翻转卷积核
        c3_conv_flip = np.flip(c3_conv, axis=(-2, -1))
        c3_conv_flip_reshaped = c3_conv_flip[:, :, np.newaxis, np.newaxis, :, :]

        c3_X_grad_mult = c3_L_pad_blocks * c3_conv_flip_reshaped

        c3_X_grad = c3_X_grad_mult.sum(axis=(-2,-1)).sum(axis=1)

        # print(c3_X_grad[0,0,0,:5])

        S2_max_pos = S2_devided.reshape(batch_size,S2_c,S2_size//S2_stride,S2_size//S2_stride,-1).argmax(axis=-1)
        S2_max_b, S2_max_c, S2_max_I, S2_max_J = np.meshgrid(np.arange(batch_size),np.arange(S2_c), np.arange(0,S2_size,S2_stride),
                                         np.arange(0,S2_size,S2_stride),indexing='ij')
        S2_max_I, S2_max_J = S2_max_pos // 2 + S2_max_I, S2_max_pos % 2 + S2_max_J
        S2_in_grad = np.zeros_like(S2_in)
        S2_in_grad[S2_max_b,S2_max_c,S2_max_I,S2_max_J] = c3_X_grad
        # print(S2_in_grad[0,0,0,:10])

        c1_z_b, c1_z_c, c1_z_h, c1_z_w = c1_z.shape
        # --- 反向传播  ---
        c1_l = S2_in_grad * (c1_z > 0).astype(np.float32())

        c1_idx_x_h = np.arange(0, c1_x_h - c1_c_h + 1, c1_stride)
        c1_idx_x_w = np.arange(0, c1_x_w - c1_c_w + 1, c1_stride)
        c1_idx_x_I, c1_idx_x_J = np.meshgrid(c1_idx_x_h, c1_idx_x_w, indexing='ij')


        c1_idx_x_I, c1_idx_x_J = np.arange(c1_c_h)[...,None,None,None] + c1_idx_x_I, np.arange(c1_c_w)[...,None,None] + c1_idx_x_J
        # print(c1_l[:, :, np.newaxis,np.newaxis, np.newaxis,:, :].shape)
        # c1_X_patches shape: (c1_x_b, 1, c1_x_c, c1_c_h, c1_c_w, c1_z_h, c1_z_w)
        # print(c1_X[:,np.newaxis,:,c1_idx_x_I, c1_idx_x_J].shape)

        c1_conv_grad = (c1_X[:,np.newaxis,:,c1_idx_x_I, c1_idx_x_J] * c1_l[:, :, np.newaxis,np.newaxis, np.newaxis,:, :]).sum(axis=(-2,-1)).sum(axis=0)
        # print(c1_conv_grad.shape)

        c1_b_grad = c1_l.sum(axis=(-2,-1)).sum(axis=0)
        # print(f'c1_b_grad.shape:{c1_b_grad.shape}')

        # c1_pad_up = c1_c_h - 1 - c1_padding
        # c1_pad_left = c1_c_w - 1 - c1_padding
        # c1_l_pad = np.zeros((c1_z_b, c1_z_c, c1_x_h + c1_c_h - 2 * c1_padding - 1, c1_x_w + c1_c_w - 2 * c1_padding - 1))
        #
        # # Indices for inserting c1_l into c1_l_pad (accounts for stride and "full" convolution needed for backprop)
        # c1_l_pad_I, c1_l_pad_J = np.meshgrid(np.arange(c1_pad_up, c1_pad_up + c1_stride * c1_z_h, c1_stride),
        #                                      np.arange(c1_pad_up,c1_pad_up + c1_stride * c1_z_w, c1_stride),indexing='ij')
        # c1_l_pad[:,:,c1_l_pad_I,c1_l_pad_J] = c1_l
        #
        # # Indices for the (padded) loss gradient patches for the 'full' convolution
        # c1_l_pad_conv_I, c1_l_pad_conv_J = np.meshgrid(np.arange(c1_c_h),np.arange(c1_c_w),indexing='ij')
        # c1_l_pad_conv_I, c1_l_pad_conv_J = np.arange(c1_x_h - 2 * c1_padding)[:,None,None,None] + c1_l_pad_conv_I, np.arange(c1_x_w - 2 * c1_padding)[:,None,None] + c1_l_pad_conv_J
        #
        # c1_X_grad = (c1_l_pad[:,:,np.newaxis,c1_l_pad_conv_I,c1_l_pad_conv_J] * np.flip(c1_conv[:,:,np.newaxis,np.newaxis,:,:],axis=(-2,-1))).sum(axis=(-2,-1)).sum(axis=1)
        # print(c1_X_grad.shape) # Expected: (32, 1, 28, 28)
        #
        # print(f"c1_conv_grad shape: {c1_conv_grad.shape}")
        # print(f"c1_X_grad shape: {c1_X_grad.shape}")
        # print(f"c1_X_grad:{c1_X_grad[0,0,0,:10]}")

        # 检查权重梯度
        # print_grad_stats('W3_loss', W3_loss)
        # print_grad_stats('W2_loss', W2_loss)
        # print_grad_stats('W1_loss', W1_loss)
        # print_grad_stats('c3_conv_grad', c3_conv_grad)
        # print_grad_stats('c1_conv_grad', c1_conv_grad)
        #
        # # 检查关键的 feature-map 梯度（判断梯度是否被阻断/变 0）
        # print_grad_stats('c3_X_grad', c3_X_grad)
        # print_grad_stats('S2_in_grad', S2_in_grad)
        # print_grad_stats('c1_l (c1 local grad)', c1_l)
        #
        # # 还可以针对中间激活检查值域（是否全部接近 0/1）
        # print_grad_stats('S2_in (act)', S2_in)
        # print_grad_stats('S4_in (act)', S4_in)

        c1_conv -= c1_conv_grad * lr
        c1_b -= c1_b_grad * lr
        c3_conv -= c3_conv_grad * lr
        c3_b -= c3_b_grad * lr
        W1 -= W1_loss * lr
        b1 -= b1_loss * lr
        W2 -= W2_loss * lr
        b2 -= b2_loss * lr
        W3 -= W3_loss * lr
        b3 -= b3_loss * lr

# import torch
# import numpy as np
# import torch.nn as nn
#
# # --- 1. 定义 NumPy 变量并转换为 PyTorch 张量 ---
#
# # 从您的 NumPy 示例中提取参数
# batch_size = 32
# input_size = 400
# hidden_size_1 = 120
# hidden_size_2 = 84
# output_size = 10
#
# # 使用您生成的W和b的值 (假设 W1, b1, ... W3, b3 已经在外部环境中定义为 NumPy 数组)
# W1_np = W1
# b1_np = b1
# W2_np = W2
# b2_np = b2
# W3_np = W3
# b3_np = b3
#
# # # 输入数据 (X1)
# # X_np = X1
# # # 将 NumPy 数组转换为 PyTorch 张量，并使用新的变量名
# # Input_X = torch.tensor(X_np, dtype=torch.float32)
# True_Y = torch.tensor(y, dtype=torch.float32)
#
# # 初始化权重和偏置作为 PyTorch 的可学习参数 (requires_grad=True)
# # 使用 M_W1, V_b1 等命名来区分 NumPy 变量
# M_W1 = torch.tensor(W1_np, dtype=torch.float32, requires_grad=True)
# V_b1 = torch.tensor(b1_np, dtype=torch.float32, requires_grad=True)
# M_W2 = torch.tensor(W2_np, dtype=torch.float32, requires_grad=True)
# V_b2 = torch.tensor(b2_np, dtype=torch.float32, requires_grad=True)
# M_W3 = torch.tensor(W3_np, dtype=torch.float32, requires_grad=True)
# V_b3 = torch.tensor(b3_np, dtype=torch.float32, requires_grad=True)
#
# # --- 2. 定义激活函数 ---
# def sigmoid(z):
#     return torch.sigmoid(z)
#
# def softmax(z):
#     # PyTorch 的 nn.Softmax 默认沿最后一个维度运行
#     return torch.softmax(z, dim=1)
#
#
# # --- Setup Parameters ---
# # The original code implies batch_size=32 and S2_c=6
# batch_size = 32
# S2_c = 6
# kernel_size = 5
# in_channels = 1
# out_channels = S2_c
# stride = 1
# padding = 2
#
#
# c1_conv_layer = nn.Conv2d(
#     in_channels=in_channels,
#     out_channels=out_channels,
#     kernel_size=kernel_size,
#     stride=stride,
#     padding=padding,
#     bias=False
# )
#
# c1_x_torch = torch.tensor(c1_x, dtype=torch.float, requires_grad=True)
#
# try:
#     # Convert numpy array to torch tensor
#     numpy_weights_tensor = torch.from_numpy(c1_conv).float().requires_grad_(True)
#     # Assign the weights to the PyTorch layer
#     c1_conv_layer.weight.data = numpy_weights_tensor
# except NameError:
#     # If c1_conv is not defined, just use the random init
#     print("Note: c1_conv numpy array not found; using PyTorch's default random weight initialization.")
# except ValueError as e:
#     print(f"Error setting weights: {e}")
#     print("Using PyTorch's default random weight initialization.")
#
#
# # --- 4. Forward Pass Execution ---
# c1_z_torch = c1_conv_layer(c1_x_torch)
#
# # --- 5. Print Results ---
# print(f"PyTorch Output Shape: {c1_z_torch.shape}")
#
# S2_c = 6       # 假设通道数 (LeNet S2 层通常是 6 或 16)
# S2_size = 28   # 输入尺寸 28x28
# S2_stride = 2
# S2_p_size = 2  # Kernel size
#
#
#
# # 转换为 PyTorch 张量
# S2_in_torch = sigmoid(c1_z_torch)
#
#
# max_pool_layer = nn.MaxPool2d(
#     kernel_size=S2_p_size,
#     stride=S2_stride
# )
#
# S2_out_torch = max_pool_layer(S2_in_torch)
#
# # --- 1. 参数定义和 PyTorch 张量初始化 ---
#
# # 从您的 NumPy 变量中提取和定义参数
# c3_in_channels = 6
# c3_out_channels = 16
# c3_H_in = 14
# c3_W_in = 14
# c3_kernel_size = 5
# c3_stride = 1
# c3_padding = 0
#
# # 假设 c3_x 和 c3_conv 已经在外部环境中定义为 NumPy 数组
# try:
#     c3_conv_np = c3_conv
# except NameError:
#     # 如果 NumPy 变量未定义，则重新创建 (用于确保代码可运行)
#     c3_conv_np = np.random.randn(c3_out_channels, c3_in_channels, c3_kernel_size, c3_kernel_size)
#
# # --- 将 NumPy 数据转换为 PyTorch 张量 ---
# # 1. 输入数据 X
# c3_X_torch = S2_out_torch
# # --- 2. 定义 PyTorch 卷积层并初始化权重 ---
#
# c3_conv_layer = nn.Conv2d(
#     in_channels=c3_in_channels,
#     out_channels=c3_out_channels,
#     kernel_size=c3_kernel_size,
#     stride=c3_stride,
#     padding=c3_padding,
#     bias=False # 您的 NumPy 代码没有偏置项，因此这里设置为 False
# )
#
# # 使用 NumPy 权重初始化 PyTorch 层的权重 (c3_conv -> c3_conv_layer.weight)
# c3_conv_layer.weight.data = torch.tensor(c3_conv_np, dtype=torch.float32)
#
#
# # --- 3. 前向传播 (Forward Pass) ---
#
# # Z_out = Conv2d(X)
# c3_Z_out_torch = c3_conv_layer(c3_X_torch)
#
# # 转换为 PyTorch 张量
# # 形状：(Batch, Channels, Height, Width)
# S4_in_torch = sigmoid(c3_Z_out_torch)
#
# # --- 2. PyTorch 均值池化层实现 ---
#
# # 1. 定义 AvgPool2d 层
# # kernel_size=2, stride=2, 且默认 padding=0
# avg_pool = nn.AvgPool2d(
#     kernel_size=S4_p_size,
#     stride=S4_stride,
#     padding=0
# )
#
# # 2. 执行均值池化 (Average Pooling)
# # 输出 S4_out_avg 对应您的 S4_out (如果它计算平均值)
# S4_out_avg = avg_pool(S4_in_torch)
#
# # --- 3. reshape 和展平 ---
#
# # S4_out_avg 的形状为 (batch_size, 16, 5, 5)
# # reshape(batch_size, -1) 将展平后两个维度
# # 最终形状应为 (32, 16 * 5 * 5) = (32, 400)
# X1_torch = S4_out_avg.reshape(batch_size, -1)
#
# # 第一层 (Layer 1)
# Z1 = torch.matmul(X1_torch, M_W1) + V_b1
# Output_X2 = sigmoid(Z1)
#
# # 第二层 (Layer 2)
# Z2 = torch.matmul(Output_X2, M_W2) + V_b2
# Output_X3 = sigmoid(Z2)
#
# # 第三层/输出层 (Layer 3/Output)
# Z3 = torch.matmul(Output_X3, M_W3) + V_b3
# Predicted_Y = softmax(Z3) # 预测的输出 Y_hat
#
# # --- 4. 损失计算 (Loss Calculation) ---
#
# # 使用 L = 0.5 * sum((Predicted_Y - True_Y)^2)
# Loss = 0.5 * ((Predicted_Y - True_Y) ** 2).sum()
#
# # --- 5. 反向传播 (Backward Pass) ---
#
# # 清除所有现有梯度
# M_W1.grad = None
# V_b1.grad = None
# M_W2.grad = None
# V_b2.grad = None
# M_W3.grad = None
# V_b3.grad = None
#
# # 计算损失对所有参数的梯度
# Loss.backward()
#
# # --- 6. 打印结果 (Print Results) ---
#
# print("Predicted_Y 的shape:", Predicted_Y.shape)
# print(f"使用的 L2 损失值: {Loss.item():.4f}")
# print("-" * 50)
# print("反向传播完成，梯度已计算。")
#
#
# print(f'c1_x_torch.grad{c1_x_torch.grad[0,0,0,:5], c1_x_torch.grad.shape}')
# print(f'c1_x_torch.grad == c1_X_grad {np.allclose(c1_x_torch.grad,c1_X_grad,rtol=0, atol=5e-5)}')