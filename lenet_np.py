# -*- coding: utf-8 -*-
"""lenet_np.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bwCEpLqUyaCTrWASWO55dE6MnGITNJAs
"""

import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def softmax(x):
    ex = np.exp(x)
    return ex / np.sum(ex, axis=1, keepdims=True)

def softmax_jacobian(x):
    if len(x.shape) == 1:
        return np.diag(x) - np.outer(x, x)
    else:
        diag = np.zeros((x.shape[0], x.shape[1], x.shape[1]))
        diag[:, np.arange(x.shape[1]), np.arange(x.shape[1])] = x
        outer = x[..., None, :] * x[...,:, None]
        return diag - outer

batch_size = 32
output_size = 10
S4_c = 16
S2_c = 6

c1_x = np.random.randn(batch_size, 1, 28, 28)
c1_conv = np.random.randn(S2_c, 1, 5, 5)
c1_stride = 1
c1_padding = 2
c1_x_b, c1_x_c, c1_x_h, c1_x_w = c1_x.shape
c1_c_b, c1_c_c, c1_c_h, c1_c_w = c1_conv.shape
c1_x_h += 2 * c1_padding
c1_x_w += 2 * c1_padding
c1_X = np.zeros((c1_x_b, c1_x_c, c1_x_h, c1_x_w))
c1_X[:,:, c1_padding:-c1_padding, c1_padding:-c1_padding] = c1_x
print(c1_c_b,c1_c_c,c1_c_h,c1_c_w) # Expected: 6 1 5 5

# --- Forward Pass (Convolution) ---
c1_h_pos = np.arange(0, c1_x_h - c1_c_h + 1, c1_stride)[:,None,None,None]
c1_w_pos = np.arange(0, c1_x_w - c1_c_w + 1, c1_stride)[:,None,None]
c1_I, c1_J = np.meshgrid(np.arange(c1_c_h), np.arange(c1_c_w), indexing='ij')

# Implemented as an explicit loop over the input/filter elements (unrolled using broadcasting/indexing)
# c1_z shape: (batch_size, num_filters, input_channels, output_h, output_w)
c1_z = (c1_X[:, np.newaxis, :, c1_h_pos+c1_I, c1_w_pos+c1_J] * c1_conv[:, :,np.newaxis,np.newaxis,:,:]).sum(axis=(-2, -1))
print(c1_z.shape) # Expected: (32, 6, 1, 28, 28) -> Batch, Filters, In_Channels, H, W

# Sum over input channels (c1_x_c), which is 1 here
c1_z = np.sum(c1_z, axis=2)
print(c1_z.shape) # Expected: (32, 6, 28, 28) -> Batch, Filters, H, W

S2_size = 28
S2_in = sigmoid(c1_z)
S2_stride = 2
S2_p_size = 2
S2_I, S2_J = np.meshgrid(np.arange(S2_p_size), np.arange(S2_p_size), indexing='ij')
S2_I, S2_J = np.arange(0,S2_size,S2_stride)[:,None,None,None] + S2_I, np.arange(0,S2_size,S2_stride)[:,None,None] + S2_J
S2_devided = S2_in[:,:,S2_I,S2_J].copy()
S2_out = S2_in[:,:,S2_I,S2_J].max(axis=(-2,-1))

c3_x = S2_out
c3_conv = np.random.randn(S4_c, 6, 5, 5)
c3_stride = 1
c3_padding = 0
c3_x_b, c3_x_c, c3_x_h, c3_x_w = c3_x.shape
c3_c_b, c3_c_c, c3_c_h, c3_c_w = c3_conv.shape

# --- 2. 前向传播 (Forward Pass) ---
c3_padded_h = c3_x_h + 2 * c3_padding
c3_padded_w = c3_x_w + 2 * c3_padding
c3_X_padded = np.zeros((c3_x_b, c3_x_c, c3_padded_h, c3_padded_w))
c3_X_padded[:, :, c3_padding : c3_padded_h - c3_padding, c3_padding : c3_padded_w - c3_padding] = c3_x
c3_H_out = (c3_padded_h - c3_c_h) // c3_stride + 1
c3_W_out = (c3_padded_w - c3_c_w) // c3_stride + 1

# 2.1 索引创建
c3_h_pos = np.arange(0, c3_padded_h - c3_c_h + 1, c3_stride)[:, None, None, None]
c3_w_pos = np.arange(0, c3_padded_w - c3_c_w + 1, c3_stride)[:, None, None]
c3_I, c3_J = np.meshgrid(np.arange(c3_c_h), np.arange(c3_c_w), indexing='ij')
c3_H_index = c3_h_pos + c3_I
c3_W_index = c3_w_pos + c3_J

# 2.2 卷积计算
c3_X_blocks = c3_X_padded[:, np.newaxis, :, c3_H_index, c3_W_index]
c3_conv_reshaped = c3_conv[:, :, np.newaxis, np.newaxis, :, :]
c3_Z_mult = c3_X_blocks * c3_conv_reshaped
c3_z = c3_Z_mult.sum(axis=(-2, -1)) # 沿着 F_h, F_w 求和 (输入通道 C_in 仍保留)
c3_Z_out = np.sum(c3_z, axis=2)

S4_size = 10
S4_in = sigmoid(c3_Z_out)
S4_stride = 2
S4_p_size = 2
S4_I, S4_J = np.meshgrid(np.arange(S4_p_size), np.arange(S4_p_size), indexing='ij')
S4_I, S4_J = np.arange(0,S4_size,S4_stride)[:,None,None,None] + S4_I, np.arange(0,S4_size,S4_stride)[:,None,None] + S4_J
S4_out = S4_in[:,:,S4_I,S4_J].mean(axis=(-2,-1))
X1 = S4_out.reshape(batch_size,-1)
W1 = np.random.randn(400, 120)
b1 = np.random.randn(120,)
z1 = X1 @ W1 + b1
z1.shape

X2 = sigmoid(z1)

W2 = np.random.randn(120,84)
b2 = np.random.randn(84,)
z2 = X2 @ W2 + b2
X3 = sigmoid(z2)
W3 = np.random.randn(84,10)
b3 = np.random.randn(10,)
z3 = X3 @ W3 + b3
y_hat = softmax(z3)
y = np.zeros((batch_size,output_size))
y_tag = np.random.randint(output_size, size=batch_size) # 为每个样本生成一个随机类别标签
y[np.arange(batch_size),y_tag] = 1

mse_loss = y_hat - y
print(((mse_loss ** 2) / 2).sum())
z3_loss = np.matmul(mse_loss[:,None,:], softmax_jacobian(y_hat)).reshape(batch_size, -1)
b3_loss = z3_loss.sum(axis=0)
W3_loss = np.matmul(X3.T,z3_loss)
X3_loss = np.matmul(z3_loss,W3.T)
z2_loss = X3_loss * X3 * (1 - X3)
b2_loss = z2_loss.sum(axis=0)
W2_loss = np.matmul(X2.T,z2_loss)
X2_loss = np.matmul(z2_loss,W2.T)
z1_loss = X2_loss * X2 * (1 - X2)
b1_loss = z1_loss.sum(axis=0)
W1_loss = np.matmul(X1.T,z1_loss)
X1_loss = np.matmul(z1_loss,W1.T)
temp_loss = X1_loss.reshape(batch_size, S4_c, S4_size // S4_stride, S4_size // S4_stride)
S4_loss = (temp_loss / (S4_p_size * S4_p_size)).repeat(2, -1).repeat(2, -2)
print(S4_loss[0,0,0,:5])
print(W1_loss[0][:5])
print(b1_loss[:5])

c3_z_b, c3_z_c, c3_z_h, c3_z_w = c3_Z_out.shape
c3_l = S4_in * (1 - S4_in) * S4_loss
c3_l.shape


# 重新构造索引，用于提取输入数据块 X 和 L
c3_idx_x_h = np.arange(0, c3_padded_h - c3_c_h + 1, c3_stride)
c3_idx_x_w = np.arange(0, c3_padded_w - c3_c_w + 1, c3_stride)
c3_idx_x_I, c3_idx_x_J = np.meshgrid(c3_idx_x_h, c3_idx_x_w, indexing='ij')

c3_rel_I, c3_rel_J = np.meshgrid(np.arange(c3_c_h), np.arange(c3_c_w), indexing='ij')

c3_final_I = c3_rel_I[..., None, None] + c3_idx_x_I
c3_final_J = c3_rel_J[..., None, None] + c3_idx_x_J

c3_L_reshaped = c3_l[:, :, np.newaxis, np.newaxis, np.newaxis, :, :]

# 提取输入数据块X的形状：(B, 1, C_in, F_h, F_w, H_out, W_out)
c3_X_blocks_for_grad = c3_X_padded[:, np.newaxis, :, c3_final_I, c3_final_J]
c3_grad_mult = c3_X_blocks_for_grad * c3_L_reshaped
c3_conv_grad = c3_grad_mult.sum(axis=(-2,-1)).sum(axis=0)

# 计算需要的填充量
c3_pad_up = c3_c_h - 1 - c3_padding
c3_pad_left = c3_c_w - 1 - c3_padding
c3_l_pad = np.zeros((c3_z_b, c3_z_c,
                     c3_x_h + c3_c_h - 2 * c3_padding - 1,
                     c3_x_w + c3_c_w - 2 * c3_padding - 1))

# 创建 L 放置到 L_pad 的索引
# H_start: c3_c_h - 1 - c3_padding = 4
c3_l_pad_I, c3_l_pad_J = np.meshgrid(
    np.arange(c3_pad_up, c3_pad_up + c3_stride * c3_z_h, c3_stride),
    np.arange(c3_pad_left, c3_pad_left + c3_stride * c3_z_w, c3_stride),
    indexing='ij'
)

# 将 L 放入 L_pad 中
c3_l_pad[:, :, c3_l_pad_I, c3_l_pad_J] = c3_l

# 2. 对 L_pad 进行卷积操作 (使用翻转后的卷积核)
c3_l_pad_conv_I, c3_l_pad_conv_J = np.meshgrid(np.arange(c3_c_h), np.arange(c3_c_w), indexing='ij')
c3_l_pad_H_index = np.arange(c3_x_h - 2 * c3_padding)[:, None, None, None] + c3_l_pad_conv_I
c3_l_pad_W_index = np.arange(c3_x_w - 2 * c3_padding)[:, None, None] + c3_l_pad_conv_J
c3_L_pad_blocks = c3_l_pad[:, :, np.newaxis, c3_l_pad_H_index, c3_l_pad_W_index]

# 翻转卷积核
c3_conv_flip = np.flip(c3_conv, axis=(-2, -1))
c3_conv_flip_reshaped = c3_conv_flip[:, :, np.newaxis, np.newaxis, :, :]

# 计算 dL/dX： (B, C_out, 1, H_in, W_in, F_h, F_w) * (C_out, C_in, 1, 1, F_h, F_w)
c3_X_grad_mult = c3_L_pad_blocks * c3_conv_flip_reshaped

# 沿 C_out (输出通道) 和 F_h, F_w 求和
# 结果形状：(B, C_out, C_in, H_in, W_in, F_h, F_w) -> (B, C_in, H_in, W_in)
c3_X_grad = c3_X_grad_mult.sum(axis=(-2,-1)).sum(axis=1)

# 打印最终输入梯度形状
print(c3_X_grad[0,0,0,:5])

S2_max_pos = S2_devided.reshape(batch_size,S2_c,S2_size//S2_stride,S2_size//S2_stride,-1).argmax(axis=-1)
S2_max_b, S2_max_c, S2_max_I, S2_max_J = np.meshgrid(np.arange(batch_size),np.arange(S2_c), np.arange(0,S2_size,S2_stride),
                                 np.arange(0,S2_size,S2_stride),indexing='ij')
S2_max_I, S2_max_J = S2_max_pos // 2 + S2_max_I, S2_max_pos % 2 + S2_max_J
S2_in_grad = np.zeros_like(S2_in)
S2_in_grad[S2_max_b,S2_max_c,S2_max_I,S2_max_J] = c3_X_grad
print(S2_in_grad[0,0,0,:10])

c1_z_b, c1_z_c, c1_z_h, c1_z_w = c1_z.shape
# --- Backward Pass (Gradient Calculation) ---
c1_l = S2_in_grad * S2_in * (1 - S2_in)
## 1. Gradient w.r.t. Filter (c1_conv_grad)

# Indices for input patches corresponding to the output gradient c1_l
c1_idx_x_h = np.arange(0, c1_x_h - c1_c_h + 1, c1_stride)
c1_idx_x_w = np.arange(0, c1_x_w - c1_c_w + 1, c1_stride)
c1_idx_x_I, c1_idx_x_J = np.meshgrid(c1_idx_x_h, c1_idx_x_w, indexing='ij')


c1_idx_x_I, c1_idx_x_J = np.arange(c1_c_h)[...,None,None,None] + c1_idx_x_I, np.arange(c1_c_w)[...,None,None] + c1_idx_x_J
print(c1_l[:, :, np.newaxis,np.newaxis, np.newaxis,:, :].shape)
# c1_X_patches shape: (c1_x_b, 1, c1_x_c, c1_c_h, c1_c_w, c1_z_h, c1_z_w)
print(c1_X[:,np.newaxis,:,c1_idx_x_I, c1_idx_x_J].shape)

c1_conv_grad = (c1_X[:,np.newaxis,:,c1_idx_x_I, c1_idx_x_J] * c1_l[:, :, np.newaxis,np.newaxis, np.newaxis,:, :]).sum(axis=(-2,-1)).sum(axis=0)
print(c1_conv_grad.shape)

c1_pad_up = c1_c_h - 1 - c1_padding
c1_pad_left = c1_c_w - 1 - c1_padding
c1_l_pad = np.zeros((c1_z_b, c1_z_c, c1_x_h + c1_c_h - 2 * c1_padding - 1, c1_x_w + c1_c_w - 2 * c1_padding - 1))

# Indices for inserting c1_l into c1_l_pad (accounts for stride and "full" convolution needed for backprop)
c1_l_pad_I, c1_l_pad_J = np.meshgrid(np.arange(c1_pad_up, c1_pad_up + c1_stride * c1_z_h, c1_stride),
                                     np.arange(c1_pad_up,c1_pad_up + c1_stride * c1_z_w, c1_stride),indexing='ij')
c1_l_pad[:,:,c1_l_pad_I,c1_l_pad_J] = c1_l

# Indices for the (padded) loss gradient patches for the 'full' convolution
c1_l_pad_conv_I, c1_l_pad_conv_J = np.meshgrid(np.arange(c1_c_h),np.arange(c1_c_w),indexing='ij')
c1_l_pad_conv_I, c1_l_pad_conv_J = np.arange(c1_x_h - 2 * c1_padding)[:,None,None,None] + c1_l_pad_conv_I, np.arange(c1_x_w - 2 * c1_padding)[:,None,None] + c1_l_pad_conv_J

c1_X_grad = (c1_l_pad[:,:,np.newaxis,c1_l_pad_conv_I,c1_l_pad_conv_J] * np.flip(c1_conv[:,:,np.newaxis,np.newaxis,:,:],axis=(-2,-1))).sum(axis=(-2,-1)).sum(axis=1)
print(c1_X_grad.shape) # Expected: (32, 1, 28, 28)

print(f"c1_conv_grad shape: {c1_conv_grad.shape}")
print(f"c1_X_grad shape: {c1_X_grad.shape}")
print(f"c1_X_grad:{c1_X_grad[0,0,0,:10]}")

import torch
import numpy as np
import torch.nn as nn

# --- 1. 定义 NumPy 变量并转换为 PyTorch 张量 ---

# 从您的 NumPy 示例中提取参数
batch_size = 32
input_size = 400
hidden_size_1 = 120
hidden_size_2 = 84
output_size = 10

# 使用您生成的W和b的值 (假设 W1, b1, ... W3, b3 已经在外部环境中定义为 NumPy 数组)
W1_np = W1
b1_np = b1
W2_np = W2
b2_np = b2
W3_np = W3
b3_np = b3

# # 输入数据 (X1)
# X_np = X1
# # 将 NumPy 数组转换为 PyTorch 张量，并使用新的变量名
# Input_X = torch.tensor(X_np, dtype=torch.float32)
True_Y = torch.tensor(y, dtype=torch.float32)

# 初始化权重和偏置作为 PyTorch 的可学习参数 (requires_grad=True)
# 使用 M_W1, V_b1 等命名来区分 NumPy 变量
M_W1 = torch.tensor(W1_np, dtype=torch.float32, requires_grad=True)
V_b1 = torch.tensor(b1_np, dtype=torch.float32, requires_grad=True)
M_W2 = torch.tensor(W2_np, dtype=torch.float32, requires_grad=True)
V_b2 = torch.tensor(b2_np, dtype=torch.float32, requires_grad=True)
M_W3 = torch.tensor(W3_np, dtype=torch.float32, requires_grad=True)
V_b3 = torch.tensor(b3_np, dtype=torch.float32, requires_grad=True)

# --- 2. 定义激活函数 ---
def sigmoid(z):
    return torch.sigmoid(z)

def softmax(z):
    # PyTorch 的 nn.Softmax 默认沿最后一个维度运行
    return torch.softmax(z, dim=1)


# --- Setup Parameters ---
# The original code implies batch_size=32 and S2_c=6
batch_size = 32
S2_c = 6
kernel_size = 5
in_channels = 1
out_channels = S2_c
stride = 1
padding = 2


c1_conv_layer = nn.Conv2d(
    in_channels=in_channels,
    out_channels=out_channels,
    kernel_size=kernel_size,
    stride=stride,
    padding=padding,
    bias=False
)

c1_x_torch = torch.tensor(c1_x, dtype=torch.float, requires_grad=True)

try:
    # Convert numpy array to torch tensor
    numpy_weights_tensor = torch.from_numpy(c1_conv).float().requires_grad_(True)
    # Assign the weights to the PyTorch layer
    c1_conv_layer.weight.data = numpy_weights_tensor
except NameError:
    # If c1_conv is not defined, just use the random init
    print("Note: c1_conv numpy array not found; using PyTorch's default random weight initialization.")
except ValueError as e:
    print(f"Error setting weights: {e}")
    print("Using PyTorch's default random weight initialization.")


# --- 4. Forward Pass Execution ---
c1_z_torch = c1_conv_layer(c1_x_torch)

# --- 5. Print Results ---
print(f"PyTorch Output Shape: {c1_z_torch.shape}")

S2_c = 6       # 假设通道数 (LeNet S2 层通常是 6 或 16)
S2_size = 28   # 输入尺寸 28x28
S2_stride = 2
S2_p_size = 2  # Kernel size



# 转换为 PyTorch 张量
S2_in_torch = sigmoid(c1_z_torch)


max_pool_layer = nn.MaxPool2d(
    kernel_size=S2_p_size,
    stride=S2_stride
)

S2_out_torch = max_pool_layer(S2_in_torch)

# --- 1. 参数定义和 PyTorch 张量初始化 ---

# 从您的 NumPy 变量中提取和定义参数
c3_in_channels = 6
c3_out_channels = 16
c3_H_in = 14
c3_W_in = 14
c3_kernel_size = 5
c3_stride = 1
c3_padding = 0

# 假设 c3_x 和 c3_conv 已经在外部环境中定义为 NumPy 数组
try:
    c3_conv_np = c3_conv
except NameError:
    # 如果 NumPy 变量未定义，则重新创建 (用于确保代码可运行)
    c3_conv_np = np.random.randn(c3_out_channels, c3_in_channels, c3_kernel_size, c3_kernel_size)

# --- 将 NumPy 数据转换为 PyTorch 张量 ---
# 1. 输入数据 X
c3_X_torch = S2_out_torch
# --- 2. 定义 PyTorch 卷积层并初始化权重 ---

c3_conv_layer = nn.Conv2d(
    in_channels=c3_in_channels,
    out_channels=c3_out_channels,
    kernel_size=c3_kernel_size,
    stride=c3_stride,
    padding=c3_padding,
    bias=False # 您的 NumPy 代码没有偏置项，因此这里设置为 False
)

# 使用 NumPy 权重初始化 PyTorch 层的权重 (c3_conv -> c3_conv_layer.weight)
c3_conv_layer.weight.data = torch.tensor(c3_conv_np, dtype=torch.float32)


# --- 3. 前向传播 (Forward Pass) ---

# Z_out = Conv2d(X)
c3_Z_out_torch = c3_conv_layer(c3_X_torch)

# 转换为 PyTorch 张量
# 形状：(Batch, Channels, Height, Width)
S4_in_torch = sigmoid(c3_Z_out_torch)

# --- 2. PyTorch 均值池化层实现 ---

# 1. 定义 AvgPool2d 层
# kernel_size=2, stride=2, 且默认 padding=0
avg_pool = nn.AvgPool2d(
    kernel_size=S4_p_size,
    stride=S4_stride,
    padding=0
)

# 2. 执行均值池化 (Average Pooling)
# 输出 S4_out_avg 对应您的 S4_out (如果它计算平均值)
S4_out_avg = avg_pool(S4_in_torch)

# --- 3. reshape 和展平 ---

# S4_out_avg 的形状为 (batch_size, 16, 5, 5)
# reshape(batch_size, -1) 将展平后两个维度
# 最终形状应为 (32, 16 * 5 * 5) = (32, 400)
X1_torch = S4_out_avg.reshape(batch_size, -1)

# 第一层 (Layer 1)
Z1 = torch.matmul(X1_torch, M_W1) + V_b1
Output_X2 = sigmoid(Z1)

# 第二层 (Layer 2)
Z2 = torch.matmul(Output_X2, M_W2) + V_b2
Output_X3 = sigmoid(Z2)

# 第三层/输出层 (Layer 3/Output)
Z3 = torch.matmul(Output_X3, M_W3) + V_b3
Predicted_Y = softmax(Z3) # 预测的输出 Y_hat

# --- 4. 损失计算 (Loss Calculation) ---

# 使用 L = 0.5 * sum((Predicted_Y - True_Y)^2)
Loss = 0.5 * ((Predicted_Y - True_Y) ** 2).sum()

# --- 5. 反向传播 (Backward Pass) ---

# 清除所有现有梯度
M_W1.grad = None
V_b1.grad = None
M_W2.grad = None
V_b2.grad = None
M_W3.grad = None
V_b3.grad = None

# 计算损失对所有参数的梯度
Loss.backward()

# --- 6. 打印结果 (Print Results) ---

print("Predicted_Y 的shape:", Predicted_Y.shape)
print(f"使用的 L2 损失值: {Loss.item():.4f}")
print("-" * 50)
print("反向传播完成，梯度已计算。")


print(f'c1_x_torch.grad{c1_x_torch.grad[0,0,0,:5], c1_x_torch.grad.shape}')
print(f'c1_x_torch.grad == c1_X_grad {np.allclose(c1_x_torch.grad,c1_X_grad,rtol=0, atol=5e-5)}')